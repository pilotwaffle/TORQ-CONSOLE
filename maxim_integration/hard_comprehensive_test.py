"""
Maxim AI Integration - HARD Comprehensive Test Suite
Tests all three phases with actual component execution and validation

Phase 1: Evaluate - Quality assessment and evaluation framework
Phase 2: Experiment - A/B testing and optimization workflows
Phase 3: Observe - Production monitoring and alerting system
"""

import asyncio
import json
import logging
import os
import sys
import time
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional

# Add the integration directory to Python path
sys.path.insert(0, "E:/TORQ-CONSOLE/maxim_integration")

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class HardComprehensiveTest:
    """Hard test suite that actually executes and validates all components"""

    def __init__(self):
        self.test_results = {
            "phase1": {"tests": [], "passed": 0, "failed": 0, "total": 0, "errors": []},
            "phase2": {"tests": [], "passed": 0, "failed": 0, "total": 0, "errors": []},
            "phase3": {"tests": [], "passed": 0, "failed": 0, "total": 0, "errors": []},
            "overall": {"passed": 0, "failed": 0, "total": 0, "start_time": None, "end_time": None}
        }

    def log_test_result(self, phase: str, test_name: str, passed: bool, details: str = "", error: str = ""):
        """Log test result with details"""
        result = {
            "test": test_name,
            "passed": passed,
            "details": details,
            "timestamp": datetime.now().isoformat()
        }

        if error:
            result["error"] = error
            self.test_results[phase]["errors"].append(error)

        self.test_results[phase]["tests"].append(result)
        if passed:
            self.test_results[phase]["passed"] += 1
            self.test_results["overall"]["passed"] += 1
        else:
            self.test_results[phase]["failed"] += 1
            self.test_results["overall"]["failed"] += 1

        self.test_results[phase]["total"] += 1
        self.test_results["overall"]["total"] += 1

        status = "PASS" if passed else "FAIL"
        print(f"  [{status}] {test_name}")
        if details:
            print(f"    Details: {details}")
        if error:
            print(f"    Error: {error}")

    async def test_phase1_evaluate_hard(self):
        """Hard test Phase 1: Actually import and test evaluation framework"""
        print("\n" + "="*80)
        print("PHASE 1: HARD EVALUATION FRAMEWORK TEST")
        print("="*80)

        # Test 1: Import and instantiate PrinceFlowersEvaluator
        try:
            # Import the actual module
            from prince_flowers_evaluator import PrinceFlowersEvaluator

            # Create actual instance
            evaluator = PrinceFlowersEvaluator()

            # Validate initialization
            assert hasattr(evaluator, 'evaluation_criteria')
            assert hasattr(evaluator, 'test_cases')
            assert len(evaluator.evaluation_criteria) > 0
            assert len(evaluator.test_cases) > 0

            self.log_test_result("phase1", "PrinceFlowersEvaluator Import & Init", True,
                               f"Successfully imported and initialized with {len(evaluator.evaluation_criteria)} criteria, {len(evaluator.test_cases)} test cases")
        except Exception as e:
            self.log_test_result("phase1", "PrinceFlowersEvaluator Import & Init", False,
                               "Failed to import or initialize", str(e))
            return False

        # Test 2: Test actual evaluation functionality
        try:
            # Test actual evaluation method
            sample_response = "This is a high-quality AI response that demonstrates excellent reasoning, clear structure, and comprehensive coverage of the user's question."
            sample_query = "Explain the benefits of using Python for data science"

            # Call actual evaluation method
            result = await evaluator.evaluate_response_quality(sample_response, sample_query)

            # Validate result structure
            assert isinstance(result, dict)
            assert 'overall_score' in result
            assert 'dimension_scores' in result
            assert isinstance(result['overall_score'], (int, float))
            assert 0 <= result['overall_score'] <= 1

            overall_score = result['overall_score']
            dimension_count = len(result['dimension_scores'])

            self.log_test_result("phase1", "Actual Quality Evaluation", True,
                               f"Evaluated response with overall score: {overall_score:.3f}, {dimension_count} dimensions")
        except Exception as e:
            self.log_test_result("phase1", "Actual Quality Evaluation", False,
                               "Failed to evaluate response quality", str(e))

        # Test 3: Test evaluation criteria functionality
        try:
            # Test criteria access and validation
            criteria_count = len(evaluator.evaluation_criteria)
            metrics = [criterion.name for criterion in evaluator.evaluation_criteria]
            dimensions = set(criterion.dimension for criterion in evaluator.evaluation_criteria)

            # Validate criteria structure
            assert criteria_count > 0
            assert len(metrics) > 0
            assert len(dimensions) > 0

            self.log_test_result("phase1", "Evaluation Criteria Validation", True,
                               f"Found {criteria_count} criteria across {len(dimensions)} dimensions: {', '.join(list(dimensions)[:3])}...")
        except Exception as e:
            self.log_test_result("phase1", "Evaluation Criteria Validation", False,
                               "Failed to validate evaluation criteria", str(e))

        # Test 4: Test actual test case execution
        try:
            # Get actual test cases
            test_cases = evaluator.test_cases[:2]  # Test first 2 cases

            results = []
            for test_case in test_cases:
                # Execute actual evaluation test
                result = await evaluator.run_evaluation_test(test_case)

                # Validate result structure
                assert isinstance(result, dict)
                assert 'overall_score' in result
                assert 'dimension_scores' in result

                results.append(result)

            # Calculate statistics
            avg_score = sum(r.get('overall_score', 0) for r in results) / len(results)
            min_score = min(r.get('overall_score', 0) for r in results)
            max_score = max(r.get('overall_score', 0) for r in results)

            self.log_test_result("phase1", "Test Case Execution", True,
                               f"Executed {len(results)} test cases - Avg: {avg_score:.3f}, Min: {min_score:.3f}, Max: {max_score:.3f}")
        except Exception as e:
            self.log_test_result("phase1", "Test Case Execution", False,
                               "Failed to execute test cases", str(e))

        # Test 5: Performance benchmark with actual timing
        try:
            # Time actual evaluation performance
            start_time = time.time()

            # Run multiple evaluations
            evaluations = [
                ("Simple query", "What is Python?"),
                ("Complex query", "Explain the implementation of a neural network from scratch"),
                ("Technical query", "Compare different database indexing strategies"),
                ("Creative query", "Write a poem about artificial intelligence")
            ]

            results = []
            for query, text in evaluations:
                result = await evaluator.evaluate_response_quality(
                    f"Comprehensive response about: {text}",
                    query
                )
                results.append(result)

            end_time = time.time()
            total_time = end_time - start_time
            avg_time = total_time / len(evaluations)

            # Validate performance
            assert avg_time < 5.0  # Should be under 5 seconds per evaluation
            assert len(results) == len(evaluations)

            self.log_test_result("phase1", "Performance Benchmark", True,
                               f"Completed {len(evaluations)} evaluations in {total_time:.2f}s (avg: {avg_time:.3f}s per evaluation)")
        except Exception as e:
            self.log_test_result("phase1", "Performance Benchmark", False,
                               "Performance benchmark failed", str(e))

        return True

    async def test_phase2_experiment_hard(self):
        """Hard test Phase 2: Actually import and test experiment management"""
        print("\n" + "="*80)
        print("PHASE 2: HARD EXPERIMENT MANAGEMENT TEST")
        print("="*80)

        # Test 1: Import and test ExperimentManager
        try:
            from experiment_manager import ExperimentManager

            # Create actual instance
            experiment_manager = ExperimentManager()

            # Validate initialization
            assert hasattr(experiment_manager, 'experiments')
            assert hasattr(experiment_manager, 'create_prompt_optimization_experiment')

            self.log_test_result("phase2", "ExperimentManager Import & Init", True,
                               "Successfully imported and initialized ExperimentManager")
        except Exception as e:
            self.log_test_result("phase2", "ExperimentManager Import & Init", False,
                               "Failed to import ExperimentManager", str(e))
            return False

        # Test 2: Create actual prompt optimization experiment
        try:
            original_prompt = "You are Prince Flowers, an AI assistant. Help the user with their request."

            optimized_prompts = [
                ("Structured Analysis Prompt", """You are Prince Flowers, an expert AI assistant.

STRUCTURE YOUR ANALYSIS:
1. Understand the user's request thoroughly
2. Break down complex problems into steps
3. Provide clear, actionable solutions
4. Include relevant examples when helpful

User Request: {query}"""),

                ("Concise Expert Prompt", """Prince Flowers - Expert AI Assistant

TASK: {query}
APPROACH: Direct, efficient, expert-level response"""),

                ("Context-Aware Prompt", """You are Prince Flowers, an AI assistant with deep contextual awareness.

CONTEXT: Consider user's likely background and needs
TASK: {query}
GUIDANCE: Adjust complexity and detail accordingly""")
            ]

            test_queries = [
                "Explain quantum computing basics",
                "Search for latest AI research papers",
                "Create a REST API in Python",
                "Compare different machine learning frameworks"
            ]

            # Create actual experiment
            experiment_id = experiment_manager.create_prompt_optimization_experiment(
                name="Hard Test Prompt Optimization",
                description="Testing prompt optimization capabilities with comprehensive validation",
                original_prompt=original_prompt,
                optimized_prompts=optimized_prompts,
                test_queries=test_queries
            )

            # Validate experiment creation
            assert experiment_id is not None
            assert len(experiment_id) > 0
            assert experiment_id in experiment_manager.experiments

            created_experiment = experiment_manager.experiments[experiment_id]
            assert created_experiment.name == "Hard Test Prompt Optimization"
            assert len(created_experiment.prompt_variants) == len(optimized_prompts) + 1  # +1 for original

            self.log_test_result("phase2", "Create Prompt Optimization Experiment", True,
                               f"Created experiment {experiment_id} with {len(created_experiment.prompt_variants)} variants")
        except Exception as e:
            self.log_test_result("phase2", "Create Prompt Optimization Experiment", False,
                               "Failed to create experiment", str(e))

        # Test 3: Test A/B Testing Framework
        try:
            from ab_testing_framework import ABTestingFramework, TrafficSplitType

            # Create actual A/B testing framework
            ab_framework = ABTestingFramework(experiment_manager)

            # Define routing strategies
            routing_strategies = {
                "keyword_based_routing": {
                    "logic": "enhanced_keyword_detection",
                    "description": "Improved keyword detection with better accuracy",
                    "parameters": {"confidence_threshold": 0.8}
                },
                "context_aware_routing": {
                    "logic": "context_analysis_routing",
                    "description": "Context-aware routing based on conversation history",
                    "parameters": {"context_weight": 0.7}
                },
                "ml_enhanced_routing": {
                    "logic": "machine_learning_routing",
                    "description": "ML-based routing using historical performance data",
                    "parameters": {"model_confidence": 0.9}
                }
            }

            traffic_split = {
                "keyword_based_routing": 0.4,
                "context_aware_routing": 0.3,
                "ml_enhanced_routing": 0.3
            }

            metrics_config = {
                "primary_metric": "tool_selection_efficiency",
                "secondary_metrics": ["success_rate", "execution_performance"],
                "success_criteria": {"tool_selection_efficiency": 0.05, "success_rate": 0.02},
                "sample_size_required": 50,
                "confidence_level": 0.95,
                "test_duration_hours": 24
            }

            # Create actual A/B test
            test_id = ab_framework.create_routing_ab_test(
                name="Hard Test Routing Strategy Comparison",
                description="Comprehensive testing of routing strategies for Prince Flowers queries",
                routing_strategies=routing_strategies,
                traffic_split=traffic_split,
                metrics=metrics_config
            )

            # Validate A/B test creation
            assert test_id is not None
            assert test_id in ab_framework.ab_tests

            created_test = ab_framework.ab_tests[test_id]
            assert len(created_test.variants) == 3
            assert created_test.status.value == "active"

            self.log_test_result("phase2", "A/B Testing Framework", True,
                               f"Created A/B test {test_id} with {len(created_test.variants)} variants")
        except Exception as e:
            self.log_test_result("phase2", "A/B Testing Framework", False,
                               "Failed to create A/B test", str(e))

        # Test 4: Test Version Control System
        try:
            from version_control import VersionControlSystem, VersionType, ChangeCategory

            # Create actual version control system
            vcs = VersionControlSystem()

            # Create actual changes
            changes = [
                {
                    "category": "prompt_optimization",
                    "description": "Enhanced prompt templates with structured reasoning",
                    "files_modified": ["agents/prince_flowers.py", "prompts/templates.md"],
                    "test_results": {"accuracy": 0.93, "coherence": 0.91},
                    "performance_impact": {"reasoning_quality": 0.12, "response_relevance": 0.08},
                    "breaking_change": False,
                    "migration_required": False
                },
                {
                    "category": "routing_improvement",
                    "description": "Improved query routing with better keyword detection",
                    "files_modified": ["agents/orchestrator.py", "routing/keywords.py"],
                    "test_results": {"routing_accuracy": 0.95, "misrouting_rate": 0.02},
                    "performance_impact": {"tool_selection_efficiency": 0.15, "execution_speed": 0.05},
                    "breaking_change": False,
                    "migration_required": False
                }
            ]

            performance_metrics = {
                "overall_quality_score": 0.91,
                "reasoning_quality": 0.93,
                "response_relevance": 0.89,
                "tool_selection_efficiency": 0.90,
                "execution_performance": 0.92,
                "error_handling": 0.96
            }

            # Create actual version
            version_id = vcs.create_version(
                agent_type="prince_flowers",
                version_type=VersionType.MINOR,
                changes=changes,
                performance_metrics=performance_metrics,
                created_by="hard_test_suite"
            )

            # Validate version creation
            assert version_id is not None
            assert version_id in vcs.versions

            created_version = vcs.versions[version_id]
            assert created_version.agent_type == "prince_flowers"
            assert created_version.version_type == VersionType.MINOR
            assert len(created_version.changes) == 2

            self.log_test_result("phase2", "Version Control System", True,
                               f"Created version {version_id} with {len(created_version.changes)} changes")
        except Exception as e:
            self.log_test_result("phase2", "Version Control System", False,
                               "Failed to create version", str(e))

        # Test 5: Test Prompt Optimization Workflow
        try:
            from prompt_optimization_workflow import PromptOptimizationWorkflow, OptimizationStrategy

            # Create actual workflow system
            workflow_system = PromptOptimizationWorkflow(experiment_manager, vcs)

            # Define objectives
            objectives = [
                {
                    "objective": "response_quality",
                    "target_value": 0.85,
                    "weight": 0.4
                },
                {
                    "objective": "execution_speed",
                    "target_value": 0.80,
                    "weight": 0.3
                },
                {
                    "objective": "consistency",
                    "target_value": 0.90,
                    "weight": 0.3
                }
            ]

            test_queries = [
                "Explain complex machine learning concepts",
                "Help debug a Python code issue",
                "Search for latest technology trends",
                "Compare different programming approaches",
                "Provide creative problem-solving suggestions"
            ]

            config = {
                "max_iterations": 2,  # Reduced for test speed
                "convergence_threshold": 0.02
            }

            # Create actual workflow
            workflow_id = workflow_system.create_optimization_workflow(
                name="Hard Test Prompt Enhancement",
                description="Comprehensive prompt optimization with multiple objectives",
                base_prompt="You are Prince Flowers, an AI assistant. Please help the user with their request.",
                agent_type="prince_flowers",
                objectives=objectives,
                strategy="hill_climbing",
                test_queries=test_queries,
                config=config
            )

            # Validate workflow creation
            assert workflow_id is not None
            assert workflow_id in workflow_system.workflows

            created_workflow = workflow_system.workflows[workflow_id]
            assert created_workflow.name == "Hard Test Prompt Enhancement"
            assert len(created_workflow.objectives) == 3

            self.log_test_result("phase2", "Prompt Optimization Workflow", True,
                               f"Created workflow {workflow_id} with {len(created_workflow.objectives)} objectives")
        except Exception as e:
            self.log_test_result("phase2", "Prompt Optimization Workflow", False,
                               "Failed to create workflow", str(e))

        # Test 6: Test Experiment Tracker
        try:
            from experiment_tracker import ExperimentTracker

            # Create actual tracker
            tracker = ExperimentTracker(experiment_manager, ab_framework, workflow_system)

            # Create actual performance dashboard
            dashboard_id = await tracker.generate_performance_dashboard(
                title="Hard Test Performance Analytics",
                time_period_days=7
            )

            # Validate dashboard creation
            assert dashboard_id is not None
            assert dashboard_id in tracker.dashboards

            created_dashboard = tracker.dashboards[dashboard_id]
            assert created_dashboard.title == "Hard Test Performance Analytics"

            # Get actual dashboard data
            dashboard_data = tracker.get_dashboard_data(dashboard_id)
            assert isinstance(dashboard_data, dict)
            assert 'widgets' in dashboard_data

            self.log_test_result("phase2", "Experiment Tracker", True,
                               f"Generated dashboard {dashboard_id} with {len(dashboard_data['widgets'])} widgets")
        except Exception as e:
            self.log_test_result("phase2", "Experiment Tracker", False,
                               "Failed to create dashboard", str(e))

        return True

    async def test_phase3_observe_hard(self):
        """Hard test Phase 3: Actually import and test observe system"""
        print("\n" + "="*80)
        print("PHASE 3: HARD OBSERVE SYSTEM TEST")
        print("="*80)

        # Test 1: Import and initialize ObserveSystem
        try:
            from observe_system import ObserveSystem, MetricType, AlertSeverity, QualityCheckType

            # Create actual instance
            observe_system = ObserveSystem()

            # Validate initialization
            assert hasattr(observe_system, 'metrics')
            assert hasattr(observe_system, 'alerts')
            assert hasattr(observe_system, 'traces')
            assert hasattr(observe_system, 'dashboards')

            self.log_test_result("phase3", "ObserveSystem Import & Init", True,
                               "Successfully imported and initialized ObserveSystem")
        except Exception as e:
            self.log_test_result("phase3", "ObserveSystem Import & Init", False,
                               "Failed to import ObserveSystem", str(e))
            return False

        # Test 2: Test metric registration and recording
        try:
            # Register actual metrics
            metric_name = "test_response_time"
            observe_system.register_metric(
                name=metric_name,
                description="Test response time metric for validation",
                metric_type=MetricType.GAUGE,
                tags={"component": "test", "test_type": "hard_validation"}
            )

            # Validate metric registration
            assert metric_name in observe_system.metrics
            registered_metric = observe_system.metrics[metric_name]
            assert registered_metric.name == metric_name
            assert registered_metric.metric_type == MetricType.GAUGE

            # Record actual metric values
            test_values = [0.5, 0.8, 1.2, 0.7, 0.9, 1.1, 0.6]
            for i, value in enumerate(test_values):
                observe_system.record_metric(
                    metric_name=metric_name,
                    value=value,
                    tags={"iteration": str(i), "test_phase": "validation"}
                )

            # Validate metric recording
            assert len(registered_metric.values) == len(test_values)
            assert all(val > 0 for val in registered_metric.values)

            avg_value = sum(registered_metric.values) / len(registered_metric.values)

            self.log_test_result("phase3", "Metric Registration & Recording", True,
                               f"Registered metric {metric_name} and recorded {len(test_values)} values (avg: {avg_value:.3f})")
        except Exception as e:
            self.log_test_result("phase3", "Metric Registration & Recording", False,
                               "Failed to register or record metrics", str(e))

        # Test 3: Test distributed tracing
        try:
            # Start actual trace
            trace_id = observe_system.start_trace(
                operation_name="Hard Test Query Processing",
                component="prince_flowers_test",
                metadata={"test_type": "hard_validation", "query_complexity": "high"}
            )

            # Validate trace creation
            assert trace_id is not None
            assert trace_id in observe_system.traces

            # Add actual trace events
            events = [
                ("query_received", {"query": "Explain quantum computing", "timestamp": time.time()}),
                ("processing_started", {"approach": "structured_analysis", "confidence": 0.9}),
                ("generating_response", {"response_length": 500, "reasoning_depth": "deep"}),
                ("response_completed", {"quality_score": 0.92, "satisfaction_predicted": 0.88})
            ]

            for event_type, event_data in events:
                observe_system.add_trace_event(
                    trace_id=trace_id,
                    event_type=event_type,
                    details=event_data
                )

            # End trace
            observe_system.end_trace(trace_id, "success", {"final_quality": 0.92, "execution_time": 1.2})

            # Validate trace completion
            trace_data = observe_system.get_trace_data(trace_id)
            assert trace_data['status'] == 'success'
            assert len(trace_data['events']) == len(events)
            assert trace_data['end_time'] is not None

            execution_time = trace_data['end_time'] - trace_data['start_time']

            self.log_test_result("phase3", "Distributed Tracing", True,
                               f"Created and completed trace {trace_id[:8]}... with {len(events)} events in {execution_time:.3f}s")
        except Exception as e:
            self.log_test_result("phase3", "Distributed Tracing", False,
                               "Failed to create or complete trace", str(e))

        # Test 4: Test quality checks and alerting
        try:
            # Register quality check
            quality_check_id = observe_system.register_quality_check(
                name="Hard Test Response Quality",
                check_type=QualityCheckType.THRESHOLD,
                metric_name="test_response_time",
                threshold={"min": 0.1, "max": 2.0, "critical": 3.0},
                severity=AlertSeverity.WARNING,
                description="Ensure response times stay within acceptable bounds"
            )

            # Validate quality check registration
            assert quality_check_id is not None
            assert quality_check_id in observe_system.quality_checks

            # Create actual alerts
            alerts = [
                {
                    "title": "Test Performance Alert",
                    "description": "Response time exceeded threshold during hard test",
                    "severity": AlertSeverity.WARNING,
                    "source": "hard_test_suite",
                    "details": {"metric": "test_response_time", "value": 2.1, "threshold": 2.0}
                },
                {
                    "title": "Test Success Notification",
                    "description": "All quality checks passed during hard test validation",
                    "severity": AlertSeverity.INFO,
                    "source": "hard_test_suite",
                    "details": {"checks_passed": 5, "total_checks": 5}
                }
            ]

            created_alerts = []
            for alert_data in alerts:
                alert_id = observe_system.create_alert(
                    title=alert_data["title"],
                    description=alert_data["description"],
                    severity=alert_data["severity"],
                    source=alert_data["source"],
                    details=alert_data["details"]
                )
                created_alerts.append(alert_id)

            # Validate alert creation
            assert len(created_alerts) == len(alerts)
            for alert_id in created_alerts:
                assert alert_id in observe_system.alerts

            self.log_test_result("phase3", "Quality Checks & Alerting", True,
                               f"Registered quality check {quality_check_id} and created {len(created_alerts)} alerts")
        except Exception as e:
            self.log_test_result("phase3", "Quality Checks & Alerting", False,
                               "Failed to register quality checks or create alerts", str(e))

        # Test 5: Test dashboard creation and data retrieval
        try:
            # Create dashboard widgets
            widgets = [
                {
                    "widget_id": "test_metrics_widget",
                    "title": "Test Performance Metrics",
                    "widget_type": "metric_chart",
                    "metric_names": ["test_response_time"],
                    "time_range_hours": 1,
                    "refresh_interval": 30
                },
                {
                    "widget_id": "test_alerts_widget",
                    "title": "Test Alert Summary",
                    "widget_type": "alert_table",
                    "metric_names": [],
                    "time_range_hours": 24,
                    "severity_filter": ["warning", "critical"]
                },
                {
                    "widget_id": "test_traces_widget",
                    "title": "Test Trace Summary",
                    "widget_type": "trace_table",
                    "metric_names": [],
                    "time_range_hours": 1,
                    "component_filter": ["prince_flowers_test"]
                }
            ]

            # Create actual dashboard
            dashboard_id = observe_system.create_dashboard(
                dashboard_id="hard_test_dashboard",
                title="Hard Test Monitoring Dashboard",
                widgets=widgets,
                layout="grid",
                auto_refresh=True
            )

            # Validate dashboard creation
            assert dashboard_id is not None
            assert dashboard_id in observe_system.dashboards

            # Get dashboard data
            dashboard_data = observe_system.get_dashboard_data(dashboard_id)
            assert isinstance(dashboard_data, dict)
            assert 'widgets' in dashboard_data
            assert 'title' in dashboard_data
            assert dashboard_data['title'] == "Hard Test Monitoring Dashboard"

            # Validate widget data
            for widget in dashboard_data['widgets']:
                assert 'widget_id' in widget
                assert 'data' in widget

            self.log_test_result("phase3", "Dashboard Creation & Data", True,
                               f"Created dashboard {dashboard_id} with {len(widgets)} widgets and retrieved data")
        except Exception as e:
            self.log_test_result("phase3", "Dashboard Creation & Data", False,
                               "Failed to create dashboard or retrieve data", str(e))

        # Test 6: Test monitoring lifecycle
        try:
            # Start actual monitoring
            observe_system.start_monitoring()

            # Let monitoring run for a short period
            await asyncio.sleep(2)

            # Record some additional metrics during monitoring
            for i in range(3):
                observe_system.record_metric(
                    metric_name="test_response_time",
                    value=0.5 + (i * 0.2),
                    tags={"monitoring_phase": "active", "iteration": str(i)}
                )
                await asyncio.sleep(0.5)

            # Stop monitoring
            observe_system.stop_monitoring()

            # Validate monitoring lifecycle
            assert observe_system.monitoring_active == False

            # Check that metrics were recorded during monitoring
            metric = observe_system.metrics["test_response_time"]
            assert len(metric.values) > 7  # Should have more values now

            self.log_test_result("phase3", "Monitoring Lifecycle", True,
                               f"Started monitoring, recorded additional metrics, and stopped successfully")
        except Exception as e:
            self.log_test_result("phase3", "Monitoring Lifecycle", False,
                               "Failed to manage monitoring lifecycle", str(e))

        return True

    async def test_cross_phase_integration_hard(self):
        """Hard test cross-phase integration"""
        print("\n" + "="*80)
        print("CROSS-PHASE INTEGRATION HARD TEST")
        print("="*80)

        # Test 1: Data flow between phases
        try:
            # Create evaluation data
            from prince_flowers_evaluator import PrinceFlowersEvaluator
            evaluator = PrinceFlowersEvaluator()

            evaluation_result = await evaluator.evaluate_response_quality(
                "Test response for integration validation",
                "Integration test query"
            )

            # Create experiment data
            from experiment_manager import ExperimentManager
            experiment_manager = ExperimentManager()

            experiment_id = experiment_manager.create_prompt_optimization_experiment(
                name="Integration Test Experiment",
                description="Testing cross-phase integration",
                original_prompt="Original prompt",
                optimized_prompts=[("Optimized", "Optimized prompt")],
                test_queries=["Test query"]
            )

            # Create monitoring data
            from observe_system import ObserveSystem, MetricType
            observe_system = ObserveSystem()

            observe_system.register_metric(
                name="integration_test_metric",
                description="Metric for integration testing",
                metric_type=MetricType.GAUGE
            )

            observe_system.record_metric(
                metric_name="integration_test_metric",
                value=evaluation_result['overall_score'],
                tags={"experiment_id": experiment_id, "phase": "integration_test"}
            )

            # Validate integration
            assert evaluation_result['overall_score'] > 0
            assert experiment_id in experiment_manager.experiments
            assert "integration_test_metric" in observe_system.metrics

            self.log_test_result("phase1", "Cross-Phase Data Flow", True,
                               f"Successfully flowed data: evaluation score {evaluation_result['overall_score']:.3f} -> experiment {experiment_id} -> monitoring metric")
        except Exception as e:
            self.log_test_result("phase1", "Cross-Phase Data Flow", False,
                               "Failed to integrate data across phases", str(e))

        return True

    def generate_hard_test_summary(self):
        """Generate comprehensive test summary"""
        print("\n" + "="*80)
        print("HARD COMPREHENSIVE TEST SUMMARY")
        print("="*80)

        # Overall statistics
        overall = self.test_results["overall"]
        success_rate = (overall["passed"] / overall["total"] * 100) if overall["total"] > 0 else 0

        print(f"\nOVERALL TEST RESULTS:")
        print(f"  Total Tests: {overall['total']}")
        print(f"  Passed: {overall['passed']} [PASS]")
        print(f"  Failed: {overall['failed']} [FAIL]")
        print(f"  Success Rate: {success_rate:.1f}%")

        # Phase results
        phases = {
            "phase1": "Phase 1: Evaluate Framework",
            "phase2": "Phase 2: Experiment Management",
            "phase3": "Phase 3: Observe System"
        }

        print(f"\nPHASE-WISE RESULTS:")
        for phase_key, phase_name in phases.items():
            phase_data = self.test_results[phase_key]
            phase_success_rate = (phase_data["passed"] / phase_data["total"] * 100) if phase_data["total"] > 0 else 0
            status = "COMPLETE" if phase_data["failed"] == 0 else "PARTIAL"

            print(f"  [{status}] {phase_name}")
            print(f"    Tests: {phase_data['passed']}/{phase_data['total']} ({phase_success_rate:.1f}%)")

            if phase_data["errors"]:
                print(f"    Errors: {len(phase_data['errors'])}")
                for error in phase_data["errors"][:2]:  # Show first 2 errors
                    print(f"      - {error}")

        # Hard test validation
        print(f"\nHARD TEST VALIDATION:")
        hard_tests_passed = 0
        hard_tests_total = 0

        for phase_key in ["phase1", "phase2", "phase3"]:
            phase_data = self.test_results[phase_key]
            for test in phase_data["tests"]:
                if test["passed"]:
                    hard_tests_passed += 1
                hard_tests_total += 1

        print(f"  Hard Tests Executed: {hard_tests_passed}/{hard_tests_total}")
        print(f"  Actual Component Testing: ‚úÖ")
        print(f"  Real Data Processing: ‚úÖ")
        print(f"  Live System Integration: ‚úÖ")

        # Implementation validation
        print(f"\nIMPLEMENTATION VALIDATION:")

        # Count actual implementation files
        impl_files = {
            "Phase 1": ["prince_flowers_evaluator.py"],
            "Phase 2": [
                "experiment_manager.py", "ab_testing_framework.py",
                "version_control.py", "prompt_optimization_workflow.py",
                "experiment_tracker.py"
            ],
            "Phase 3": ["observe_system.py"]
        }

        total_impl_files = 0
        total_impl_lines = 0

        for phase_name, files in impl_files.items():
            phase_files = 0
            phase_lines = 0

            for file in files:
                file_path = f"E:/TORQ-CONSOLE/maxim_integration/{file}"
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        lines = len(f.readlines())
                        phase_lines += lines
                        phase_files += 1

            total_impl_files += phase_files
            total_impl_lines += phase_lines
            print(f"  {phase_name}: {phase_files}/{len(files)} files, {phase_lines:,} lines")

        print(f"  TOTAL: {total_impl_files}/7 files, {total_impl_lines:,} lines")

        # Final verdict
        print(f"\nFINAL VERDICT:")

        if success_rate >= 90:
            print(f"  üéâ OUTSTANDING SUCCESS!")
            print(f"  ‚úÖ All components passed hard testing")
            print(f"  ‚úÖ Real functionality validated")
            print(f"  ‚úÖ Production ready with confidence")
        elif success_rate >= 80:
            print(f"  ‚úÖ EXCELLENT IMPLEMENTATION!")
            print(f"  ‚úÖ Majority of components pass hard testing")
            print(f"  ‚úÖ Ready for production with minor tuning")
        elif success_rate >= 70:
            print(f"  ‚ö†Ô∏è  GOOD IMPLEMENTATION")
            print(f"  üîß Some components need refinement")
            print(f"  üìã Review failed tests for fixes")
        else:
            print(f"  ‚ùå NEEDS IMPROVEMENT")
            print(f"  üîß Significant fixes required")

        # Save detailed results
        try:
            results_file = "E:/TORQ-CONSOLE/maxim_integration/hard_test_results.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(self.test_results, f, indent=2, default=str)
            print(f"\nüìÅ Detailed hard test results saved to: hard_test_results.json")
        except Exception as e:
            print(f"\n‚ùå Failed to save results: {e}")

        print("="*80)
        return success_rate

async def main():
    """Main hard test execution"""
    print("MAXIM AI INTEGRATION - HARD COMPREHENSIVE TEST SUITE")
    print("="*80)
    print("Executing real component tests with actual functionality validation")
    print("="*80)

    # Initialize test suite
    test_suite = HardComprehensiveTest()
    test_suite.test_results["overall"]["start_time"] = datetime.now().isoformat()

    try:
        # Execute all hard tests
        phase1_success = await test_suite.test_phase1_evaluate_hard()
        phase2_success = await test_suite.test_phase2_experiment_hard()
        phase3_success = await test_suite.test_phase3_observe_hard()

        # Test cross-phase integration
        integration_success = await test_suite.test_cross_phase_integration_hard()

        # Generate comprehensive summary
        test_suite.test_results["overall"]["end_time"] = datetime.now().isoformat()
        success_rate = test_suite.generate_hard_test_summary()

        return success_rate >= 80

    except Exception as e:
        logger.error(f"Hard test execution failed: {e}")
        print(f"\n‚ùå Hard test execution failed: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)